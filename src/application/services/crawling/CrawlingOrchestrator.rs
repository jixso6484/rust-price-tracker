use std::time::{Duration, Instant, SystemTime};
use std::collections::HashMap;
use std::sync::Arc;
use headless_chrome::Tab;

// Infrastructure ì˜ì¡´ì„±
use crate::infrastruction::llm::llmRepository::LocalLLM;
use crate::infrastruction::browser::chromiumAdapter::ChromiumAdapter;
use crate::infrastruction::browser::models::{BrowserAction, BrowserState};
use crate::infrastruction::html::basic::{ParserFactory, HtmlParser};
use crate::infrastruction::database::models::Product;
use crate::infrastruction::llm::models::MarketplaceStructure;

// ì—ëŸ¬ ì²˜ë¦¬
use crate::components::error::error_cl::Result;

#[derive(Debug)]
pub struct CrawlingOrchestrator {
    /// ë…ë¦½ LLM ì¸ìŠ¤í„´ìŠ¤ 
    llm: Arc<LocalLLM>,
    
    /// ë…ë¦½ ë¸Œë¼ìš°ì € ì¸ìŠ¤í„´ìŠ¤ (ì‚¬ì´íŠ¸ë³„ ì „ìš©)
    browser: ChromiumAdapter,
    
    /// í˜„ì¬ í™œì„± íƒ­ (ì¬ì‚¬ìš©)
    current_tab: Option<Arc<Tab>>,
    
    /// HTML íŒŒì„œ
    parser: Box<dyn HtmlParser>,

    /// ëª¨ë‹ˆí„°ë§ ì •ë³´
    monitoring: Monitoring,
    
    /// í¬ë¡¤ë§ ëŒ€ìƒ ì‚¬ì´íŠ¸ë“¤
    sites: Vec<MarketplaceStructure>,
}

#[derive(Debug)]
struct Monitoring {
    // ì‹¤í–‰ í†µê³„
    pub total_requests: u64,        // ì´ ìš”ì²­ ìˆ˜
    pub successful_requests: u64,   // ì„±ê³µí•œ ìš”ì²­ ìˆ˜
    pub failed_requests: u32,       // ì‹¤íŒ¨í•œ ìš”ì²­ ìˆ˜
    pub consecutive_failures: u32,  // ì—°ì† ì‹¤íŒ¨ ìˆ˜
    
    // ë°ì´í„° ìˆ˜ì§‘ í†µê³„
    pub items_collected: u64,       // ìˆ˜ì§‘ëœ ì•„ì´í…œ ìˆ˜
    pub duplicate_items: u64,       // ì¤‘ë³µ ì•„ì´í…œ ìˆ˜
    pub new_items: u64,            // ìƒˆë¡œìš´ ì•„ì´í…œ ìˆ˜
    
    // ì„±ëŠ¥ ë©”íŠ¸ë¦­
    pub avg_response_time: Duration,    // í‰ê·  ì‘ë‹µ ì‹œê°„
    pub requests_per_minute: f64,       // ë¶„ë‹¹ ìš”ì²­ ìˆ˜
    pub last_request_time: Option<Instant>, // ë§ˆì§€ë§‰ ìš”ì²­ ì‹œê°„
    
    // ì‹œê°„ ì •ë³´
    pub started_at: Option<SystemTime>,     // ì‹œì‘ ì‹œê°„
    pub last_success_at: Option<SystemTime>, // ë§ˆì§€ë§‰ ì„±ê³µ ì‹œê°„
    pub last_failure_at: Option<SystemTime>, // ë§ˆì§€ë§‰ ì‹¤íŒ¨ ì‹œê°„
    pub next_scheduled_at: Option<SystemTime>, // ë‹¤ìŒ ì˜ˆì • ì‹œê°„
    
    // ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤
    pub memory_usage: u64,          // í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (bytes)
    pub peak_memory_usage: u64,     // ìµœëŒ€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
    
    // ì—ëŸ¬ ì„¸ë¶€ ì •ë³´
    pub error_counts: HashMap<String, u32>, // ì—ëŸ¬ íƒ€ì…ë³„ ì¹´ìš´íŠ¸
    pub last_error: Option<String>,         // ë§ˆì§€ë§‰ ì—ëŸ¬ ë©”ì‹œì§€
    
    // ì„¤ì • ì •ë³´
    pub crawl_interval: Duration,   // í¬ë¡¤ë§ ê°„ê²©
    pub timeout: Duration,          // íƒ€ì„ì•„ì›ƒ
    pub max_retries: u32,          // ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
    pub current_retry: u32,        // í˜„ì¬ ì¬ì‹œë„ ì¹´ìš´íŠ¸
    
    // í¬ë¡¤ë§ ìƒíƒœ
    pub is_crawling: bool,         // í¬ë¡¤ë§ ì¤‘ ì—¬ë¶€
}

impl Default for Monitoring {
    fn default() -> Self {
        Self {
            total_requests: 0,
            successful_requests: 0,
            failed_requests: 0,
            consecutive_failures: 0,
            items_collected: 0,
            duplicate_items: 0,
            new_items: 0,
            avg_response_time: Duration::from_secs(0),
            requests_per_minute: 0.0,
            last_request_time: None,
            started_at: None,
            last_success_at: None,
            last_failure_at: None,
            next_scheduled_at: None,
            memory_usage: 0,
            peak_memory_usage: 0,
            error_counts: HashMap::new(),
            last_error: None,
            crawl_interval: Duration::from_secs(60),
            timeout: Duration::from_secs(30),
            max_retries: 3,
            current_retry: 0,
            is_crawling: false,
        }
    }
}

impl CrawlingOrchestrator {
    /// ì‚¬ì´íŠ¸ë³„ ë…ë¦½ í¬ë¡¤ë§ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ìƒì„±
    pub async fn new(sites: Vec<MarketplaceStructure>) -> Result<Self> {
        let llm = LocalLLM::get_instance().await?;
        let browser = ChromiumAdapter::new().await?;
        
        // ì²« ë²ˆì§¸ ì‚¬ì´íŠ¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ íŒŒì„œ ìƒì„±
        let parser = if !sites.is_empty() {
            ParserFactory::get_parser(&sites[0].platform.domain)?
        } else {
            return Err(crate::components::error::error_cl::ErrorS::data(
                "CrawlingOrchestrator::new",
                "No sites provided"
            ));
        };
        
        println!("ğŸš€ ë…ë¦½ í¬ë¡¤ë§ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ìƒì„± ì™„ë£Œ");
        
        Ok(Self {
            llm,
            browser,
            current_tab: None,
            parser,
            monitoring: Monitoring::default(),
            sites,
        })
    }
    
    /// í¬ë¡¤ë§ ì‹œì‘
    pub async fn start_crawling(&mut self) -> Result<()> {
        self.monitoring.is_crawling = true;
        self.monitoring.started_at = Some(SystemTime::now());
        println!("âœ… í¬ë¡¤ë§ ì‹œì‘");
        
        // í¬ë¡¤ë§ ë¡œì§ êµ¬í˜„
        Ok(())
    }
    
    /// í¬ë¡¤ë§ ì¤‘ì§€
    pub async fn stop_crawling(&mut self) {
        self.monitoring.is_crawling = false;
        println!("ğŸ›‘ í¬ë¡¤ë§ ì¤‘ì§€");
    }
    
    /// ì—°ì† í¬ë¡¤ë§ ì‹¤í–‰
    pub async fn crawl_continuously(&mut self) -> Result<Vec<Product>> {
        let mut all_products = Vec::new();
        
        for site in self.sites.clone() {
            println!("ğŸŒ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹œì‘: {}", site.platform.name);
            
            // ì‚¬ì´íŠ¸ URLë¡œ ì´ë™
            let tab = self.browser.new_page(&site.platform.domain).await?;
            self.current_tab = Some(tab.clone());
            
            // í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
            tokio::time::sleep(Duration::from_secs(3)).await;
            
            // HTML ê°€ì ¸ì˜¤ê¸°
            let state = self.browser.execute_action(&tab, BrowserAction::GetPageState).await?;
            
            if let Some(html) = state.html {
                // ìƒí’ˆ ëª©ë¡ íŒŒì‹±
                let product_urls = self.parser.parse_product_list(&html).await?;
                println!("ğŸ“¦ {}ê°œ ìƒí’ˆ URL ë°œê²¬", product_urls.len());
                
                // ê° ìƒí’ˆ ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ (ìµœëŒ€ 10ê°œë§Œ)
                for url in product_urls.iter().take(10) {
                    println!("ğŸ” ìƒí’ˆ í˜ì´ì§€ í¬ë¡¤ë§: {}", url);
                    
                    let product_tab = self.browser.new_page(url).await?;
                    tokio::time::sleep(Duration::from_secs(2)).await;
                    
                    let product_state = self.browser.execute_action(&product_tab, BrowserAction::GetPageState).await?;
                    
                    if let Some(product_html) = product_state.html {
                        match self.parser.parse_product(&product_html, url).await {
                            Ok(product) => {
                                println!("âœ… ìƒí’ˆ ìˆ˜ì§‘: {}", product.product_name);
                                all_products.push(product);
                                self.monitoring.items_collected += 1;
                            },
                            Err(e) => {
                                println!("âŒ ìƒí’ˆ íŒŒì‹± ì‹¤íŒ¨: {}", e);
                                self.monitoring.failed_requests += 1;
                            }
                        }
                    }
                    
                    // Rate limiting
                    tokio::time::sleep(Duration::from_secs(1)).await;
                }
            }
        }
        
        println!("ğŸ“Š ì´ {}ê°œ ìƒí’ˆ ìˆ˜ì§‘ ì™„ë£Œ", all_products.len());
        Ok(all_products)
    }
    
    /// ëª¨ë‹ˆí„°ë§ ì •ë³´ ì¡°íšŒ
    pub fn get_monitoring(&self) -> &Monitoring {
        &self.monitoring
    }
}